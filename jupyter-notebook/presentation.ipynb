{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5cf306f",
   "metadata": {
    "id": "b5cf306f"
   },
   "source": [
    "# <font color=blue><center>Oil Well Use Case with HBase, Phoenix and Spark</center></font>\n",
    "## Agenda\n",
    "\n",
    "### Use Case\n",
    "In this use case we will look at an application that monitors oil wells. Sensors in oil rigs generate streaming data, which is processed by Spark and stored in HBase, for use by various analytical and reporting tools.\n",
    "\n",
    "### Architecture\n",
    "- Overview of data flow\n",
    "- Tech Stack\n",
    "\n",
    "### Environment Setup\n",
    "- AWS EC2 instance and security group creation\n",
    "- Docker installation and running\n",
    "- Usage of docker-composer and starting all the tools\n",
    "- How to access tools in local machine\n",
    "\n",
    "### Deep dive - HBase\n",
    "- Introduction \n",
    "- Data Model\n",
    "- Terminology\n",
    "- Architecture\n",
    "- CRUD operations using CLI\n",
    "- HBase Spark Connectors\n",
    "\n",
    "### Know Phoenix\n",
    "- What is Apache Phoenix?\n",
    "- Architecture\n",
    "- Data Model \n",
    "- Queries\n",
    "\n",
    "### Data Set up\n",
    "- Download Dataset\n",
    "- Table creation\n",
    "\n",
    "### Extraction\n",
    "- Stream json Files\n",
    "- Write data to HBase\n",
    "\n",
    "### Transformation and Load\n",
    "- Calculate Aggregations\n",
    "- Phoenix Table creation\n",
    "\n",
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f85ac0",
   "metadata": {
    "id": "44f85ac0"
   },
   "source": [
    "## <font color=blue>Architecture</font>\n",
    "### Overview of data flow\n",
    "#### Data Flow Architecture\n",
    "![alt text](images/oil_well_uc.png)\n",
    "### Tech Stack\n",
    "* AWS EC2\n",
    "* Docker\n",
    "* Scala\n",
    "* HBase\n",
    "* Spark SQL\n",
    "* Spark Structured Streaming\n",
    "* HDFS\n",
    "* Phoenix\n",
    "* SBT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe037b0",
   "metadata": {
    "id": "cbe037b0"
   },
   "source": [
    "## <font color=blue>Environment Setup</font>\n",
    "### AWS EC2 instance and security group creation\n",
    "- t2.xlarge instance\n",
    "- 32GB of storage recommended\n",
    "- Allow ports 4000 - 38888\n",
    "- Connect to ec2 via ssh\n",
    " <code>ssh -i \"D:\\path\\to\\private\\key.pem\" user@Public_DNS</code>\n",
    " <br/>Example:<code>ssh -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\" ec2-user@ec2-54-203-235-65.us-west-2.compute.amazonaws.com</code><br/>\n",
    "- Port forwarding \n",
    " <code>ssh -i \"D:\\path\\to\\private\\key.pem\" user@Public_DNS</code>\n",
    " <br/>Example:<code>ssh -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\" ec2-user@ec2-34-208-254-29.us-west-2.compute.amazonaws.com -L 2081:localhost:2041 -L 4888:localhost:4888 -L 2080:localhost:2080 -L 8050:localhost:8050 -L 4141:localhost:4141</code><br/>\n",
    "- Copy from local to ec2\n",
    "  <code>scp -r -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\"</code>\n",
    "  <br/>Example:<code>scp -r -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\" D:\\Users\\pyerravelly\\Downloads\\spark-standalone-cluster-on-docker-master\\build\\docker\\docker-exp ec2-user@ec2-34-208-254-29.us-west-2.compute.amazonaws.com:/home/ec2-user/docker_exp\n",
    "</code>\n",
    "\n",
    "### Docker installation and running\n",
    "    \n",
    "### Usage of docker-composer and starting all the tools\n",
    "\n",
    "- Commands to install Docker\n",
    "\n",
    "<code>sudo yum update -y</code>\n",
    "<code><br/>sudo yum install docker</code>\n",
    "<code><br/>sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose</code>\n",
    "<code><br/>sudo chmod +x /usr/local/bin/docker-compose</code>\n",
    "<code><br/>sudo gpasswd -a $USER docker</code>\n",
    "<code><br/>newgrp docker</code>\n",
    "<br/>Start Docker: <code>sudo systemctl start docker</code>\n",
    "<br/>Stop Docker: <code>sudo systemctl stop docker</code>\n",
    "\n",
    "- How to access tools in local machine <br/>\n",
    "    List Docker containers running: <code>docker ps</code><br/>\n",
    "    CLI access in Docker container: <code>docker exec -i -t kafka bash</code><br/>\n",
    "    Jupyter Lab at: http://localhost:4888/lab? <br/>\n",
    "    HDFS at: http://localhost:50070/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1152ec",
   "metadata": {
    "id": "5e1152ec"
   },
   "source": [
    "## <font color=blue> Deep dive - HBase</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a5d52",
   "metadata": {
    "id": "642a5d52"
   },
   "source": [
    "### Introduction\n",
    " Pinterest, Goibibo and Facebook Messenger etc use HBase for data storage.\n",
    " \n",
    "### Need for HBase\n",
    " Drawbacks of HDFS:\n",
    "- No Schema \n",
    "- No Random Access\n",
    "- High Latency\n",
    "- No ACID support\n",
    "\n",
    "### Data Model\n",
    "- Column Oriented\n",
    "![alt text](images/hbase-1.PNG)\n",
    "\n",
    "![alt text](images/hbase-2.PNG)\n",
    "\n",
    "![alt text](images/hbase-3.PNG)\n",
    "\n",
    "- Denormalized\n",
    "![alt text](images/hbase-4.PNG)\n",
    "\n",
    "- Only CRUD operations\n",
    "- ACID at row level\n",
    "\n",
    "### Architecture\n",
    "![alt text](images/HBase+Architecture.jpg)\n",
    "\n",
    "1. HMaster\n",
    "- Monitoring\n",
    "- Failover controlling\n",
    "- DDL operations and alters\n",
    "2. Region Server\n",
    "- Block cache\n",
    "- MemStore\n",
    "- Write Ahead Log\n",
    "- HFile\n",
    "3. Zookeeper\n",
    "- Client communication\n",
    "- Track Server failovers\n",
    "- Config info\n",
    "\n",
    "### Terminology\n",
    "- HBase Tables\n",
    "- HBase Row\n",
    "- RowKey\n",
    "- Columns\n",
    "- Column Family\n",
    "\n",
    "### CRUD operations and other Commands using CLI\n",
    "<code>docker exec -i -t h_hbase-phoenix bash</br>start-hbase.sh</br>hbase shell</code>\n",
    "#### Data Definition Language\n",
    "- create - Creates a table.\n",
    "<code>create 'personal','personal_data'</code>\n",
    "- list - Lists all the tables in HBase.\n",
    "<code>list</code>\n",
    "- disable - Disables a table.\n",
    "<code>disable 'personal'</code>\n",
    "- is_disabled - Verifies whether a table is disabled.\n",
    "<code>is_disabled 'personal'</code>\n",
    "- enable - Enables a table.\n",
    "<code>enable 'personal'</code>\n",
    "- is_enabled - Verifies whether a table is enabled.\n",
    "<code>is_enabled 'personal'</code>\n",
    "- describe - Provides the description of a table.\n",
    "<code>describe 'personal'</code>\n",
    "- alter - Alters a table.\n",
    "<code>alter 'personal', {NAME=> 'column_name'}</code>\n",
    "- exists - Verifies whether a table exists.\n",
    "<code></code>\n",
    "- drop - Drops a table from HBase.\n",
    "<code>drop 'personal'</code>\n",
    "- disable_all - Disables the tables matching the regex give in the command.\n",
    "<code> disable_all 'p.*'</code>\n",
    "- drop_all - Drops the tables matching the regex given in the command.\n",
    "<code>drop_all 'p.*'</code>\n",
    "\n",
    "#### Data Manipulation Language\n",
    "- put - Puts a cell value at a specified column in a specified row in a particular table.\n",
    "<code>\n",
    "put 'personal','2','personal_data:name','Ram'\n",
    "put 'personal','2','personal_data:city','Bengaluru'\n",
    "put 'personal','2','personal_data:age' ,'25'</code>\n",
    "- get - Fetches the contents of row or a cell.\n",
    "<code>get 'personal','2', {COLUMN => ['personal_data:name']}</code>\n",
    "- delete - Deletes a cell value in a table.\n",
    "<code>delete 'personal','2', 'personal_data:name'</code>\n",
    "- deleteall - Deletes all the cells in a given row.\n",
    "<code> deleteall 'personal',2</code>\n",
    "- scan - Scans and returns the table data.\n",
    "<code>scan 'personal'\n",
    "scan 'personal',{LIMIT=>0}</code>\n",
    "- count - Counts and returns the number of rows in a table.\n",
    "<code>count 'personal'</code>\n",
    "- truncate - Disables, drops, and recreates a specified table.\n",
    "<code>truncate 'personal'</code>\n",
    "\n",
    "### HBase Spark Connectors\n",
    "- Hive\n",
    "- Phoenix\n",
    "- HBase Spark connector\n",
    "- SHC\n",
    "- Happybase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8960077",
   "metadata": {
    "id": "f8960077"
   },
   "source": [
    "## <font color=blue>Know Phoenix</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f57ec4",
   "metadata": {
    "id": "89f57ec4"
   },
   "source": [
    "### What is Apache Phoenix?\n",
    "- Query engine\n",
    "- Metadata repository\n",
    "- JDBC driver\n",
    "\n",
    "### Architecture\n",
    "\n",
    "![alt text](images/hbase_phoenix.png)\n",
    "\n",
    "### Performance\n",
    "- SQL Queries to native HBase scans\n",
    "- determines optiam start and stops\n",
    "- orchestrates parallel execution\n",
    "- computation to data by\n",
    "  - pushing the predicates\n",
    "  - aggregations queries through server-side hooks\n",
    "  - secondary indexes\n",
    "  - stats gathering\n",
    "  - skip scan filter\n",
    "  - optional salting\n",
    "\n",
    "### Data Model - Example\n",
    "\n",
    "\n",
    "![alt text](images/hbase-phoenix-dm.PNG)\n",
    "\n",
    "#### Mapping Existing HBase Tables\n",
    "- Phoenix supports read only access to existing HBase tables\n",
    "  - Create a Phoenix table using CREATE TABLE\n",
    "  - Or create a view using CREATE VIEW\n",
    "  - Use appropriate quoting for mixed case HBase table and native column names\n",
    "\n",
    "### Queries\n",
    "Enter Phoenix/HBase Container <code>docker exec -i -t h_hbase-phoenix bash</code>\n",
    "\n",
    "- Check whether HBase daemons are up\n",
    "<code>jps<br/>\n",
    "start-hbase.sh<br/>\n",
    "</code>\n",
    "\n",
    "- HBase to Phoenix\n",
    "<code>\n",
    "CREATE view \"personal\"(\n",
    "    ROWKEY VARCHAR PRIMARY KEY,\n",
    "    \"personal_data\".\"name\" VARCHAR,\n",
    "    \"personal_data\".\"city\" VARCHAR,\n",
    "    \"personal_data\".\"age\"  VARCHAR\n",
    ");\n",
    "</code>\n",
    "- Create sql file\n",
    "<code>\n",
    "vi us_population.sql\n",
    "    CREATE TABLE IF NOT EXISTS us_population (\n",
    "      state CHAR(2) NOT NULL,\n",
    "      city VARCHAR NOT NULL,\n",
    "      population BIGINT\n",
    "      CONSTRAINT my_pk PRIMARY KEY (state, city));</code>\n",
    "- Create data file\n",
    "<code>\n",
    "vi us_population.csv<br/>\n",
    "NY,New York,8143197\n",
    "CA,Los Angeles,3844829\n",
    "IL,Chicago,2842518\n",
    "TX,Houston,2016582\n",
    "PA,Philadelphia,1463281\n",
    "AZ,Phoenix,1461575\n",
    "TX,San Antonio,1256509\n",
    "CA,San Diego,1255540\n",
    "TX,Dallas,1213825\n",
    "CA,San Jose,912332</code>\n",
    "\n",
    "- Create table and inser data\n",
    "<code>psql.py zookeeper:2181 us_population.sql us_population.csv</code>\n",
    "\n",
    "- Enter Phoenix shell and Query\n",
    "<code>sqlline.py zookeeper:2181<br/>\n",
    "SELECT state as \"State\",count(city) as \"City Count\",sum(population) as \"Population Sum\"\n",
    "FROM us_population\n",
    "GROUP BY state\n",
    "ORDER BY sum(population) DESC;<br/>\n",
    "ctrl+D<br/></code>\n",
    "\n",
    "- Check whether created table exists in Hbase\n",
    "<code>hbase shell<br/>\n",
    "list\n",
    "</code>\n",
    "\n",
    "- Please read this document for more details about Phoenix https://phoenix.apache.org/presentations/OC-HUG-2014-10-4x3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82351959",
   "metadata": {
    "id": "82351959"
   },
   "source": [
    "## <font color=blue>Data Set up</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f35ac",
   "metadata": {
    "id": "361f35ac"
   },
   "source": [
    "- Download Dataset\n",
    "<code>docker exec -it h_namenode bash<br/>curl https://raw.githubusercontent.com/caroljmcdonald/SparkStreamingHBaseExample/master/data/sensordata.csv -o sensordata.csv<br/>ResourceID,Date,Time,HZ,Displace,Flow,SedimentPPM,PressureLbs,ChlorinePPM<br/>hdfs dfs -mkdir /input/gcoil/<br/>hdfs dfs -copyFromLocal sensordata.csv /input/gcoil/</code>\n",
    "- Table creation\n",
    "<code>jps<br/>start-hbase.sh<br/>create 'sensor', {NAME=>'data'}, {NAME=>'alert'}, {NAME=>'stats'}</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f88e11",
   "metadata": {
    "id": "74f88e11"
   },
   "source": [
    "## <font color=blue>Extraction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5c21d",
   "metadata": {
    "id": "8fe5c21d"
   },
   "source": [
    "### Read HBase table from Spark\n",
    "\n",
    "<code>docker exec -i -t h_spark-master bash</code>\n",
    "- Execute code present in spark-shell.txt\n",
    "\n",
    "### Stream json Files\n",
    "### Write data to HBase\n",
    "### Packaging\n",
    "<code>sbt package</code>\n",
    "\n",
    "- Spark Submit\n",
    "<code>\n",
    "  CLASS_NAME=GCOil\n",
    "  SPARK_JAR_NAME=/opt/workspace/project_gcoil/target/scala-2.12/gcoil-usecase_2.12-1.0.jar\n",
    "  ./spark/bin/spark-submit \\\n",
    "  --jars /opt/workspace/project_gcoil/lib/hbase-spark-1.0.1-SNAPSHOT.jar,/opt/hbase-2.4.8/lib/*.jar \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode cluster \\\n",
    "  --class $CLASS_NAME $SPARK_JAR_NAME\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b98a9",
   "metadata": {
    "id": "f58b98a9"
   },
   "source": [
    "## <font color=blue>Transformation and Load</font>\n",
    "\n",
    "- Calculate Aggregations\n",
    "<code>\n",
    "  CLASS_NAME=GCOil_Batch\n",
    "  SPARK_JAR_NAME=/opt/workspace/project_gcoil/target/scala-2.12/gcoil-usecase_2.12-1.0.jar\n",
    "  ./spark/bin/spark-submit \\\n",
    "  --jars /opt/workspace/project_gcoil/lib/hbase-spark-1.0.1-SNAPSHOT.jar,/opt/hbase-2.4.8/lib/*.jar \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode cluster \\\n",
    "  --class $CLASS_NAME $SPARK_JAR_NAME\n",
    "</code>\n",
    "Kill all the application if there are any YARN issues\n",
    "<code>\n",
    "    yarn application -list | awk '{ print $1 }' > applications_list.txt\n",
    "    while read p; do\n",
    "      echo $p\n",
    "      yarn application -kill $p\n",
    "     done < applications_list.txt\n",
    "</code>\n",
    "- Phoenix Table creation\n",
    "<code>\n",
    "CREATE view \"sensor\"(\n",
    "ROWKEY VARCHAR PRIMARY KEY,\n",
    "\"data\".\"resID\" varchar,\n",
    "\"data\".\"date\" varchar,\n",
    "\"data\".\"hz\" varchar,\n",
    "\"data\".\"disp\" varchar,\n",
    "\"data\".\"flo\" varchar,\n",
    "\"data\".\"sedPPM\" varchar,\n",
    "\"data\".\"psi\" varchar,\n",
    "\"data\".\"chlPPM\" varchar\n",
    ");\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf2769",
   "metadata": {
    "id": "8adf2769"
   },
   "source": [
    "## <font color=blue>Project Overview</font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "presentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
